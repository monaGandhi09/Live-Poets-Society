{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "This notebook contains all the steps we performed for data preprocessing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from lxml import etree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import time\n",
    "import urllib\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_json(\"goodreads_books_poetry.json\", lines=True)\n",
    "df_interactions = pd.read_json(\"goodreads_interactions_poetry.json\", lines=True)\n",
    "df_reviews = pd.read_json(\"goodreads_reviews_poetry.json\", lines=True)\n",
    "df_authors = pd.read_json(\"goodreads_book_authors.json\", lines=True)\n",
    "df_series = pd.read_json(\"goodreads_book_series.json\", lines=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping\n",
    "The `df_books` dataframe contains an `image_url` column with links to the book cover images. On manual inspection we noticed that several of the links were broken, and that the broken links were those that contained the substring \"nophoto\" in the URL. As shown below, 15861 links were broken, which is ~43% of the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of broken links: 15861\n",
      "No. of broken links (as a % of the no. of books): 43.44\n"
     ]
    }
   ],
   "source": [
    "print(f\"No. of broken links: {sum(df_books.image_url.str.contains('nophoto'))}\")\n",
    "print(f\"No. of broken links (as a % of the no. of books): {(sum(df_books.image_url.str.contains('nophoto')) / len(df_books) * 100):.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that displaying the images adds substantial value to the aesthetics of the web application. Hence we wrote a script that replaces the broken links with links to the book cover images scraped from Goodreads. This was possible as `df_books` also contains a `link` column that links to the Goodreads page of the book. Our script retrieves the Goodreads page HTMLs and parses the book cover image links using xpath.\n",
    "\n",
    "The script is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache to store links to book cover images\n",
    "image_urls = [None for _ in range(len(df_books))]\n",
    "# No. of attempts for each book. This is needed as sometimes the request fails\n",
    "# to retrieve the HTML page due to network issues\n",
    "max_tries = 3\n",
    "# Running count of no. of books with unbroken/successfully scraped links\n",
    "success_count = 0\n",
    "# Running count of no. of books with unsuccessfully scraped links \n",
    "failure_count = 0\n",
    "# No. of iterations after which to save the image URLs to disk\n",
    "checkpoint_every = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, row in df_books[[\"link\", \"image_url\"]].iterrows():\n",
    "    if \"nophoto\" not in row[\"image_url\"]: # Link is unbroken\n",
    "        image_urls[i] = row[\"image_url\"]\n",
    "    else:\n",
    "        tries = 0\n",
    "\n",
    "        # Retrieve the Goodreads page HTML and scrape the book cover image link\n",
    "        # using xpath\n",
    "        while image_urls[i] is None and tries < max_tries:\n",
    "            try:\n",
    "                response = urllib.request.urlopen(row[\"link\"])\n",
    "                tree = etree.HTML(response.read().decode(\"utf-8\"))\n",
    "                image_url = tree.xpath(\"//img[@class='ResponsiveImage' and @role='presentation']/@src\")[0]\n",
    "                image_urls[i] = image_url\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            tries += 1\n",
    "\n",
    "    if image_urls[i] is not None: # Link is now unbroken\n",
    "        success_count += 1\n",
    "    else: # Link is still broken\n",
    "        failure_count += 1\n",
    "    \n",
    "    if (i + 1) % checkpoint_every == 0: # Save checkpoint\n",
    "        with open(f\"checkpoint_{i + 1}.pickle\", mode=\"wb\") as f:\n",
    "            pickle.dump(image_urls, f)\n",
    "    \n",
    "    if i % 100 == 0: # Print logs\n",
    "        print(f\"Completed for link at index {i} | #success: {success_count}, #failures: {failure_count}, elapsed: {((time.time() - start_time) / 60):.2f} mins\")\n",
    "\n",
    "with open(f\"checkpoint_final.pickle\", mode=\"wb\") as f:\n",
    "    pickle.dump(image_urls, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the script only 2 of the links remained broken. \n",
    "\n",
    "Also, we decided not to download each of the images as that would have taken a substantial amount of time. Instead, to display the images our application simply fetches the images from the links at runtime."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `Book` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load book cover image URLs\n",
    "with open(f\"checkpoint_final.pickle\", mode=\"rb\") as f:\n",
    "    image_urls_ = pickle.load(f)\n",
    "df_books[\"image_url\"] = pd.Series(image_urls_)\n",
    "\n",
    "# Create copy of dataframe\n",
    "df_books_cleaned = df_books.copy()\n",
    "\n",
    "# Print column data types\n",
    "print(df_books_cleaned.dtypes)\n",
    "print(\"\")\n",
    "\n",
    "# Keep neccessary columns\n",
    "df_books_cleaned = df_books_cleaned[[\"book_id\", \"title\", \"description\", \"language_code\", \"edition_information\", \"format\", \"is_ebook\", \"isbn\", \"isbn13\", \"asin\", \"kindle_asin\", \"publisher\", \"num_pages\", \"image_url\"]]\n",
    "\n",
    "# Create publish_date column\n",
    "df_books_cleaned[\"publish_date\"] = pd.to_datetime(df_books.apply(lambda row: f\"{row['publication_year']}-{row['publication_month']}-{row['publication_day']}\" if all([row['publication_year'] != \"\", row['publication_month'] != \"\", row['publication_day'] != \"\", (row['publication_year'] != \"\" and int(row['publication_year']) <= 2022)]) else \"\", axis=1), infer_datetime_format=True, errors=\"coerce\")\n",
    "\n",
    "# Check book_id range\n",
    "print(df_books_cleaned[\"book_id\"].min())\n",
    "print(df_books_cleaned[\"book_id\"].max())\n",
    "print(\"\")\n",
    "\n",
    "# Clean title column\n",
    "df_books_cleaned[\"title\"] = df_books_cleaned[\"title\"].apply(lambda s: s.strip(\" '\"))\n",
    "df_books_cleaned[\"title\"] = df_books_cleaned[\"title\"].apply(lambda s: s.strip('\"'))\n",
    "print(df_books_cleaned[\"title\"].apply(lambda s: len(s)).min())\n",
    "print(df_books_cleaned[\"title\"].apply(lambda s: len(s)).max())\n",
    "df_books_cleaned[\"title\"] = df_books_cleaned[\"title\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
    "df_books_cleaned[\"title\"] = df_books_cleaned[\"title\"].apply(lambda s: s.replace(\"\\r\", \"\"))\n",
    "df_books_cleaned[\"title\"] = df_books_cleaned[\"title\"].apply(lambda s: s.replace(\"\\t\", \"\"))\n",
    "print(sum(df_books_cleaned[\"title\"].apply(lambda s: len(s) == 0)))\n",
    "# Remove books with no title\n",
    "df_books_cleaned = df_books_cleaned[df_books_cleaned[\"title\"].apply(lambda s: len(s) != 0)]\n",
    "print(\"\")\n",
    "\n",
    "# Clean description column\n",
    "df_books_cleaned[\"description\"] = df_books_cleaned[\"description\"].apply(lambda s: s.strip(\" '\"))\n",
    "df_books_cleaned[\"description\"] = df_books_cleaned[\"description\"].apply(lambda s: s.strip('\"'))\n",
    "print(df_books_cleaned[\"description\"].apply(lambda s: len(s)).min())\n",
    "print(df_books_cleaned[\"description\"].apply(lambda s: len(s)).max())\n",
    "df_books_cleaned[\"description\"] = df_books_cleaned[\"description\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
    "df_books_cleaned[\"description\"] = df_books_cleaned[\"description\"].apply(lambda s: s.replace(\"\\r\", \"\"))\n",
    "df_books_cleaned[\"description\"] = df_books_cleaned[\"description\"].apply(lambda s: s.replace(\"\\t\", \"\"))\n",
    "print(sum(df_books_cleaned[\"description\"].apply(lambda s: len(s) == 0)))\n",
    "print(\"\")\n",
    "\n",
    "# Clean language_code column\n",
    "df_books_cleaned[\"language_code\"] = df_books_cleaned[\"language_code\"].apply(lambda s: s.strip(\" '\"))\n",
    "df_books_cleaned[\"language_code\"] = df_books_cleaned[\"language_code\"].apply(lambda s: s.strip('\"'))\n",
    "print(df_books_cleaned[\"language_code\"].apply(lambda s: len(s)).min())\n",
    "print(df_books_cleaned[\"language_code\"].apply(lambda s: len(s)).max())\n",
    "df_books_cleaned[\"language_code\"] = df_books_cleaned[\"language_code\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
    "df_books_cleaned[\"language_code\"] = df_books_cleaned[\"language_code\"].apply(lambda s: s.replace(\"\\r\", \"\"))\n",
    "df_books_cleaned[\"language_code\"] = df_books_cleaned[\"language_code\"].apply(lambda s: s.replace(\"\\t\", \"\"))\n",
    "print(sum(df_books_cleaned[\"language_code\"].apply(lambda s: len(s) == 0)))\n",
    "print(\"\")\n",
    "\n",
    "# Clean edition_information column\n",
    "df_books_cleaned[\"edition_information\"] = df_books_cleaned[\"edition_information\"].apply(lambda s: s.strip(\" '\"))\n",
    "df_books_cleaned[\"edition_information\"] = df_books_cleaned[\"edition_information\"].apply(lambda s: s.strip('\"'))\n",
    "print(df_books_cleaned[\"edition_information\"].apply(lambda s: len(s)).min())\n",
    "print(df_books_cleaned[\"edition_information\"].apply(lambda s: len(s)).max())\n",
    "df_books_cleaned[\"edition_information\"] = df_books_cleaned[\"edition_information\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
    "df_books_cleaned[\"edition_information\"] = df_books_cleaned[\"edition_information\"].apply(lambda s: s.replace(\"\\r\", \"\"))\n",
    "df_books_cleaned[\"edition_information\"] = df_books_cleaned[\"edition_information\"].apply(lambda s: s.replace(\"\\t\", \"\"))\n",
    "print(sum(df_books_cleaned[\"edition_information\"].apply(lambda s: len(s) == 0)))\n",
    "print(\"\")\n",
    "\n",
    "# Clean format column\n",
    "df_books_cleaned[\"format\"] = df_books_cleaned[\"format\"].apply(lambda s: s.strip(\" '\"))\n",
    "df_books_cleaned[\"format\"] = df_books_cleaned[\"format\"].apply(lambda s: s.strip('\"'))\n",
    "print(df_books_cleaned[\"format\"].apply(lambda s: len(s)).min())\n",
    "print(df_books_cleaned[\"format\"].apply(lambda s: len(s)).max())\n",
    "df_books_cleaned[\"format\"] = df_books_cleaned[\"format\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
    "df_books_cleaned[\"format\"] = df_books_cleaned[\"format\"].apply(lambda s: s.replace(\"\\r\", \"\"))\n",
    "df_books_cleaned[\"format\"] = df_books_cleaned[\"format\"].apply(lambda s: s.replace(\"\\t\", \"\"))\n",
    "print(sum(df_books_cleaned[\"format\"].apply(lambda s: len(s) == 0)))\n",
    "print(\"\")\n",
    "\n",
    "# Clean is_ebook column\n",
    "print(set(df_books_cleaned[\"is_ebook\"]))\n",
    "df_books_cleaned[\"is_ebook\"] = df_books_cleaned[\"is_ebook\"].apply(lambda s: True if s == \"true\" else False)\n",
    "print(\"\")\n",
    "\n",
    "def is_valid_code(code: str, req_len: int, valid_chars: set[str]) -> bool:\n",
    "    return len(code) == req_len and all(c in valid_chars for c in code)\n",
    "# Clean isbn column\n",
    "valid_chars: set[str] = set(string.digits + \"X\")\n",
    "df_books_cleaned[\"isbn\"] = df_books_cleaned[\"isbn\"].apply(\n",
    "    lambda code: str(code) if is_valid_code(code, 10, valid_chars) else \"\"\n",
    ")\n",
    "# Clean isbn13 column\n",
    "df_books_cleaned[\"isbn13\"] = df_books_cleaned[\"isbn13\"].apply(\n",
    "    lambda code: str(code) if is_valid_code(code, 13, valid_chars) else \"\"\n",
    ")\n",
    "# Clean asin column\n",
    "valid_chars: set[str] = set(string.ascii_uppercase + string.digits)\n",
    "df_books_cleaned[\"asin\"] = df_books_cleaned[\"asin\"].apply(\n",
    "    lambda code: str(code) if is_valid_code(code, 10, valid_chars) else \"\"\n",
    ")\n",
    "# Clean kindle_asin column\n",
    "df_books_cleaned[\"kindle_asin\"] = df_books_cleaned[\"kindle_asin\"].apply(\n",
    "    lambda code: str(code) if is_valid_code(code, 10, valid_chars) else \"\"\n",
    ")\n",
    "print(sum(df_books_cleaned[\"isbn\"].apply(lambda s: s == \"\")))\n",
    "print(sum(df_books_cleaned[\"isbn13\"].apply(lambda s: s == \"\")))\n",
    "print(sum(df_books_cleaned[\"asin\"].apply(lambda s: s == \"\")))\n",
    "print(sum(df_books_cleaned[\"kindle_asin\"].apply(lambda s: s == \"\")))\n",
    "print(\"\")\n",
    "\n",
    "# Clean publisher column\n",
    "df_books_cleaned[\"publisher\"] = df_books_cleaned[\"publisher\"].apply(lambda s: s.strip(\" '\"))\n",
    "df_books_cleaned[\"publisher\"] = df_books_cleaned[\"publisher\"].apply(lambda s: s.strip('\"'))\n",
    "print(df_books_cleaned[\"publisher\"].apply(lambda s: len(s)).min())\n",
    "print(df_books_cleaned[\"publisher\"].apply(lambda s: len(s)).max())\n",
    "df_books_cleaned[\"publisher\"] = df_books_cleaned[\"publisher\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
    "df_books_cleaned[\"publisher\"] = df_books_cleaned[\"publisher\"].apply(lambda s: s.replace(\"\\r\", \"\"))\n",
    "df_books_cleaned[\"publisher\"] = df_books_cleaned[\"publisher\"].apply(lambda s: s.replace(\"\\t\", \"\"))\n",
    "print(sum(df_books_cleaned[\"publisher\"].apply(lambda s: len(s) == 0)))\n",
    "print(\"\")\n",
    "\n",
    "# Clean num_pages column\n",
    "df_books_cleaned[\"num_pages\"] = df_books_cleaned[\"num_pages\"].apply(lambda s: int(s) if (s != \"\" and int(s) > 0) else None)\n",
    "\n",
    "# Rename columns\n",
    "df_books_cleaned = df_books_cleaned.rename(columns={\"book_id\": \"id\", \"edition_information\": \"edition\"})\n",
    "\n",
    "# Save to disk\n",
    "df_books_cleaned.to_csv(\"book.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `Similar_Books` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_book_ids = set(df_books_cleaned[\"id\"])\n",
    "df_books_ = df_books[df_books[\"book_id\"].apply(lambda x: x in valid_book_ids)][[\"book_id\", \"similar_books\"]]\n",
    "\n",
    "d = {\"book_id1\": [], \"book_id2\": []}\n",
    "\n",
    "for i, row in df_books_.iterrows():\n",
    "    for similar_book in row[\"similar_books\"]:\n",
    "        if int(similar_book) in valid_book_ids:\n",
    "            d[\"book_id1\"].append(row[\"book_id\"])\n",
    "            d[\"book_id2\"].append(int(similar_book))\n",
    "            d[\"book_id2\"].append(row[\"book_id\"])\n",
    "            d[\"book_id1\"].append(int(similar_book))\n",
    "\n",
    "# Create dataframe\n",
    "similar_books = pd.DataFrame.from_dict(d)\n",
    "\n",
    "# Drop duplicate rows\n",
    "similar_books = similar_books.drop_duplicates()\n",
    "\n",
    "# Print column data types\n",
    "print(similar_books.dtypes)\n",
    "\n",
    "# Save to disk\n",
    "similar_books.to_csv(\"similar_books.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `Series` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of dataframe\n",
    "df_series_cleaned = df_series.copy()\n",
    "\n",
    "# Keep neccessary columns\n",
    "df_series_cleaned = df_series_cleaned[[\"series_id\", \"title\", \"description\", \"numbered\"]]\n",
    "\n",
    "# Print column data types\n",
    "print(df_series_cleaned.dtypes)\n",
    "print(\"\")\n",
    "\n",
    "# Clean series_id column\n",
    "print(len(df_series_cleaned))\n",
    "print(df_series_cleaned[\"series_id\"].nunique())\n",
    "print(df_series_cleaned[\"series_id\"].min())\n",
    "print(df_series_cleaned[\"series_id\"].max())\n",
    "valid_book_ids = set(df_books_cleaned[\"id\"])\n",
    "df_books_ = df_books[df_books[\"book_id\"].apply(lambda x: x in valid_book_ids)]\n",
    "valid_series_ids = set(int(series_id) for ls in df_books_[\"series\"] for series_id in ls)\n",
    "df_series_cleaned = df_series_cleaned[df_series_cleaned[\"series_id\"].apply(lambda x: x in valid_series_ids)]\n",
    "print(\"\")\n",
    "\n",
    "# Clean title column\n",
    "df_series_cleaned[\"title\"] = df_series_cleaned[\"title\"].apply(lambda s: s.strip(\" '\"))\n",
    "df_series_cleaned[\"title\"] = df_series_cleaned[\"title\"].apply(lambda s: s.strip('\"'))\n",
    "print(df_series_cleaned[\"title\"].apply(lambda s: len(s)).max())\n",
    "df_series_cleaned[\"title\"] = df_series_cleaned[\"title\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
    "df_series_cleaned[\"title\"] = df_series_cleaned[\"title\"].apply(lambda s: s.replace(\"\\r\", \"\"))\n",
    "df_series_cleaned[\"title\"] = df_series_cleaned[\"title\"].apply(lambda s: s.replace(\"\\t\", \"\"))\n",
    "print(sum(df_series_cleaned[\"title\"].apply(lambda s: len(s) == 0)))\n",
    "# Remove series with no title\n",
    "df_series_cleaned = df_series_cleaned[df_series_cleaned[\"title\"].apply(lambda s: len(s) != 0)]\n",
    "print(\"\")\n",
    "\n",
    "# Clean description column\n",
    "df_series_cleaned[\"description\"] = df_series_cleaned[\"description\"].apply(lambda s: s.strip(\" '\"))\n",
    "df_series_cleaned[\"description\"] = df_series_cleaned[\"description\"].apply(lambda s: s.strip('\"'))\n",
    "print(df_series_cleaned[\"description\"].apply(lambda s: len(s)).max())\n",
    "df_series_cleaned[\"description\"] = df_series_cleaned[\"description\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
    "df_series_cleaned[\"description\"] = df_series_cleaned[\"description\"].apply(lambda s: s.replace(\"\\r\", \"\"))\n",
    "df_series_cleaned[\"description\"] = df_series_cleaned[\"description\"].apply(lambda s: s.replace(\"\\t\", \"\"))\n",
    "print(sum(df_series_cleaned[\"description\"].apply(lambda s: len(s) == 0)))\n",
    "print(\"\")\n",
    "\n",
    "# Clean numbered column\n",
    "print(set(df_series_cleaned[\"numbered\"]))\n",
    "df_series_cleaned[\"numbered\"] = df_series_cleaned[\"numbered\"].apply(lambda s: True if s == \"true\" else False)\n",
    "print(\"\")\n",
    "\n",
    "# Rename columns\n",
    "df_series_cleaned = df_series_cleaned.rename(columns={\"series_id\": \"id\"})\n",
    "\n",
    "# Save to disk\n",
    "df_series_cleaned.to_csv(\"series.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `In_Series` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_book_ids = set(df_books_cleaned[\"id\"])\n",
    "valid_series_ids = set(df_series_cleaned[\"id\"])\n",
    "df_books_ = df_books[df_books[\"book_id\"].apply(lambda x: x in valid_book_ids)][[\"book_id\", \"series\"]]\n",
    "\n",
    "d = {\"book_id\": [], \"series_id\": []}\n",
    "\n",
    "for i, row in df_books_.iterrows():\n",
    "    for series_id in row[\"series\"]:\n",
    "        if int(series_id) in valid_series_ids:\n",
    "            d[\"book_id\"].append(row[\"book_id\"])\n",
    "            d[\"series_id\"].append(int(series_id))\n",
    "\n",
    "# Create dataframe\n",
    "in_series = pd.DataFrame.from_dict(d)\n",
    "\n",
    "# Drop duplicate rows\n",
    "in_series = in_series.drop_duplicates()\n",
    "\n",
    "# Print column data types\n",
    "print(in_series.dtypes)\n",
    "\n",
    "# Save to disk\n",
    "in_series.to_csv(\"in_series.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `Author` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of dataframe\n",
    "df_authors_cleaned = df_authors.copy()\n",
    "\n",
    "# Keep neccessary columns\n",
    "df_authors_cleaned = df_authors_cleaned[[\"author_id\", \"name\"]]\n",
    "\n",
    "# Print column data types\n",
    "print(df_authors_cleaned.dtypes)\n",
    "print(\"\")\n",
    "\n",
    "# Clean author_id column\n",
    "print(len(df_authors_cleaned))\n",
    "print(df_authors_cleaned[\"author_id\"].nunique())\n",
    "print(df_authors_cleaned[\"author_id\"].min())\n",
    "print(df_authors_cleaned[\"author_id\"].max())\n",
    "valid_book_ids = set(df_books_cleaned[\"id\"])\n",
    "df_books_ = df_books[df_books[\"book_id\"].apply(lambda x: x in valid_book_ids)]\n",
    "valid_author_ids = set(int(d[\"author_id\"]) for ls in df_books_[\"authors\"] for d in ls)\n",
    "df_authors_cleaned = df_authors_cleaned[df_authors_cleaned[\"author_id\"].apply(lambda x: x in valid_author_ids)]\n",
    "print(\"\")\n",
    "\n",
    "# Clean name column\n",
    "df_authors_cleaned[\"name\"] = df_authors_cleaned[\"name\"].apply(lambda s: s.strip(\" '\"))\n",
    "df_authors_cleaned[\"name\"] = df_authors_cleaned[\"name\"].apply(lambda s: s.strip('\"'))\n",
    "print(df_authors_cleaned[\"name\"].apply(lambda s: len(s)).max())\n",
    "df_authors_cleaned[\"name\"] = df_authors_cleaned[\"name\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
    "df_authors_cleaned[\"name\"] = df_authors_cleaned[\"name\"].apply(lambda s: s.replace(\"\\r\", \"\"))\n",
    "df_authors_cleaned[\"name\"] = df_authors_cleaned[\"name\"].apply(lambda s: s.replace(\"\\t\", \"\"))\n",
    "print(sum(df_authors_cleaned[\"name\"].apply(lambda s: len(s) == 0)))\n",
    "# Remove authors with no name\n",
    "df_authors_cleaned = df_authors_cleaned[df_authors_cleaned[\"name\"].apply(lambda s: len(s) != 0)]\n",
    "print(\"\")\n",
    "\n",
    "# Rename columns\n",
    "df_authors_cleaned = df_authors_cleaned.rename(columns={\"author_id\": \"id\"})\n",
    "\n",
    "# Save to disk\n",
    "df_authors_cleaned.to_csv(\"author.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `Written_By` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_book_ids = set(df_books_cleaned[\"id\"])\n",
    "df_books_ = df_books[df_books[\"book_id\"].apply(lambda x: x in valid_book_ids)][[\"book_id\", \"authors\"]]\n",
    "\n",
    "d = {\"book_id\": [], \"author_id\": []}\n",
    "\n",
    "for i, row in df_books_.iterrows():\n",
    "    for d_ in row[\"authors\"]:\n",
    "        d[\"book_id\"].append(row[\"book_id\"])\n",
    "        d[\"author_id\"].append(int(d_[\"author_id\"]))\n",
    "\n",
    "# Create dataframe\n",
    "written_by = pd.DataFrame.from_dict(d)\n",
    "\n",
    "# Drop duplicate rows\n",
    "written_by = written_by.drop_duplicates()\n",
    "\n",
    "# Print column data types\n",
    "print(written_by.dtypes)\n",
    "\n",
    "# Save to disk\n",
    "written_by.to_csv(\"written_by.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `Series_By` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_book_ids = set(df_books_cleaned[\"id\"])\n",
    "valid_author_ids = set(df_authors_cleaned[\"id\"])\n",
    "valid_series_ids = set(df_series_cleaned[\"id\"])\n",
    "df_books_ = df_books[df_books[\"book_id\"].apply(lambda x: x in valid_book_ids)][[\"authors\", \"series\"]]\n",
    "\n",
    "d = {\"author_id\": [], \"series_id\": []}\n",
    "\n",
    "for i, row in df_books_.iterrows():\n",
    "    for d_ in row[\"authors\"]:\n",
    "        for series_id in row[\"series\"]:\n",
    "            if int(d_[\"author_id\"]) in valid_author_ids and int(series_id) in valid_series_ids:\n",
    "                d[\"author_id\"].append(int(d_[\"author_id\"]))\n",
    "                d[\"series_id\"].append(int(series_id))\n",
    "\n",
    "# Create dataframe\n",
    "series_by = pd.DataFrame.from_dict(d)\n",
    "\n",
    "# Drop duplicate rows\n",
    "series_by = series_by.drop_duplicates()\n",
    "\n",
    "# Print column data types\n",
    "print(series_by.dtypes)\n",
    "\n",
    "# Save to disk\n",
    "series_by.to_csv(\"series_by.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `In_Library` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of dataframe\n",
    "in_library = df_interactions.copy()\n",
    "\n",
    "# Print column data types\n",
    "print(in_library.dtypes)\n",
    "print(\"\")\n",
    "\n",
    "# Check that (user_id, book_id) pairs are unique\n",
    "in_library.groupby([\"book_id\", \"user_id\"], as_index=False).count()[\"review_id\"].max()\n",
    "\n",
    "# Keep neccessary columns\n",
    "in_library = in_library[[\"user_id\", \"book_id\", \"date_added\", \"read_at\", \"started_at\"]]\n",
    "\n",
    "# Check user_id range\n",
    "print(in_library[\"user_id\"].apply(lambda s: len(s)).min())\n",
    "print(in_library[\"user_id\"].apply(lambda s: len(s)).max())\n",
    "print(\"\")\n",
    "\n",
    "# Clean book_id column\n",
    "valid_book_ids = set(df_books_cleaned[\"id\"])\n",
    "in_library = in_library[in_library[\"book_id\"].apply(lambda x: x in valid_book_ids)]\n",
    "print(in_library[\"book_id\"].min())\n",
    "print(in_library[\"book_id\"].max())\n",
    "print(\"\")\n",
    "\n",
    "# Clean date_added column\n",
    "print(sum(in_library[\"date_added\"].apply(lambda s: len(s) == 0)))\n",
    "in_library[\"date_added\"] = pd.to_datetime(in_library[\"date_added\"], infer_datetime_format=True, errors=\"coerce\")\n",
    "print(\"\")\n",
    "\n",
    "# Clean read_at column\n",
    "print(sum(in_library[\"read_at\"].apply(lambda s: len(s) == 0)))\n",
    "in_library[\"read_at\"] = pd.to_datetime(in_library[\"read_at\"], infer_datetime_format=True, errors=\"coerce\")\n",
    "print(\"\")\n",
    "\n",
    "# Clean started_at column\n",
    "print(sum(in_library[\"started_at\"].apply(lambda s: len(s) == 0)))\n",
    "in_library[\"started_at\"] = pd.to_datetime(in_library[\"started_at\"], infer_datetime_format=True, errors=\"coerce\")\n",
    "print(\"\")\n",
    "\n",
    "# Save to disk\n",
    "in_library.to_csv(\"in_library.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `User` Dataset\n",
    "We only had access to the user IDs. Hence we used the [Faker](https://faker.readthedocs.io/en/master/) library to create dummy data for the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep neccessary columns\n",
    "df_users_cleaned = in_library[[\"user_id\"]]\n",
    "\n",
    "# Drop duplicate rows and reset index\n",
    "df_users_cleaned = df_users_cleaned.drop_duplicates()\n",
    "df_users_cleaned = df_users_cleaned.reset_index(drop=True)\n",
    "\n",
    "# Create fake information\n",
    "fake = Faker()\n",
    "len_df = len(df_users_cleaned)\n",
    "# Ensure that usernames are unique\n",
    "df_users_cleaned[\"username\"] = pd.Series(fake.unique.user_name() for _ in range(len_df))\n",
    "df_users_cleaned[\"password\"] = pd.Series(fake.password() for _ in range(len_df))\n",
    "df_users_cleaned[\"name\"] = pd.Series(fake.name() for _ in range(len_df))\n",
    "df_users_cleaned[\"email\"] = pd.Series(fake.email() for _ in range(len_df))\n",
    "\n",
    "# Rename columns\n",
    "df_users_cleaned = df_users_cleaned.rename(columns={\"user_id\": \"id\"})\n",
    "\n",
    "# Save to disk\n",
    "df_users_cleaned.to_csv(\"user.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `Review` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of dataframe\n",
    "df_reviews_cleaned = df_reviews.copy()\n",
    "\n",
    "# Keep neccessary columns\n",
    "df_reviews_cleaned = df_reviews_cleaned[[\"review_id\", \"user_id\", \"book_id\", \"review_text\", \"rating\", \"n_votes\", \"n_comments\"]]\n",
    "\n",
    "# Print column data types\n",
    "print(df_reviews_cleaned.dtypes)\n",
    "print(\"\")\n",
    "\n",
    "# Check review_id column range and no. of unique values\n",
    "print(len(df_reviews_cleaned))\n",
    "print(df_reviews_cleaned[\"review_id\"].nunique())\n",
    "print(df_reviews_cleaned[\"review_id\"].apply(lambda s: len(s)).min())\n",
    "print(df_reviews_cleaned[\"review_id\"].apply(lambda s: len(s)).max())\n",
    "print(\"\")\n",
    "\n",
    "# Clean book_id column\n",
    "valid_book_ids = set(df_books_cleaned[\"id\"])\n",
    "df_reviews_cleaned = df_reviews_cleaned[df_reviews_cleaned[\"book_id\"].apply(lambda x: x in valid_book_ids)]\n",
    "print(df_reviews_cleaned[\"book_id\"].min())\n",
    "print(df_reviews_cleaned[\"book_id\"].max())\n",
    "print(\"\")\n",
    "\n",
    "# Clean user_id column\n",
    "valid_user_ids = set(df_users_cleaned[\"id\"])\n",
    "df_reviews_cleaned = df_reviews_cleaned[df_reviews_cleaned[\"user_id\"].apply(lambda x: x in valid_user_ids)]\n",
    "print(df_reviews_cleaned[\"user_id\"].apply(lambda s: len(s)).min())\n",
    "print(df_reviews_cleaned[\"user_id\"].apply(lambda s: len(s)).max())\n",
    "print(\"\")\n",
    "\n",
    "# Clean review_text column\n",
    "df_reviews_cleaned[\"review_text\"] = df_reviews_cleaned[\"review_text\"].apply(lambda s: s.strip(\" '\"))\n",
    "df_reviews_cleaned[\"review_text\"] = df_reviews_cleaned[\"review_text\"].apply(lambda s: s.strip('\"'))\n",
    "print(df_reviews_cleaned[\"review_text\"].apply(lambda s: len(s)).max())\n",
    "df_reviews_cleaned[\"review_text\"] = df_reviews_cleaned[\"review_text\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
    "df_reviews_cleaned[\"review_text\"] = df_reviews_cleaned[\"review_text\"].apply(lambda s: s.replace(\"\\r\", \"\"))\n",
    "df_reviews_cleaned[\"review_text\"] = df_reviews_cleaned[\"review_text\"].apply(lambda s: s.replace(\"\\t\", \"\"))\n",
    "print(sum(df_reviews_cleaned[\"review_text\"].apply(lambda s: len(s) == 0)))\n",
    "print(\"\")\n",
    "\n",
    "# Clean rating column\n",
    "print(sum(df_reviews_cleaned[\"rating\"].apply(lambda x: x < 0 or x > 5)))\n",
    "print(sum(df_reviews_cleaned[\"rating\"].apply(lambda x: x == 0)))\n",
    "df_reviews_cleaned[\"rating\"] = df_reviews_cleaned[\"rating\"].apply(lambda x: x if x != 0 else np.nan)\n",
    "print(\"\")\n",
    "\n",
    "# Clean n_votes column\n",
    "print(sum(df_reviews_cleaned[\"n_votes\"].apply(lambda x: x < 0)))\n",
    "df_reviews_cleaned[\"n_votes\"] = df_reviews_cleaned[\"n_votes\"].apply(lambda x: x if x >= 0 else 0)\n",
    "print(\"\")\n",
    "\n",
    "# Clean n_comments column\n",
    "print(sum(df_reviews_cleaned[\"n_comments\"].apply(lambda x: x < 0)))\n",
    "df_reviews_cleaned[\"n_comments\"] = df_reviews_cleaned[\"n_comments\"].apply(lambda x: x if x >= 0 else 0)\n",
    "print(\"\")\n",
    "\n",
    "# Rename columns\n",
    "df_reviews_cleaned = df_reviews_cleaned.rename(columns={\"review_id\": \"id\", \"review_text\": \"text\", \"n_votes\": \"num_votes\", \"n_comments\": \"num_comments\"})\n",
    "\n",
    "# Save to disk\n",
    "df_reviews_cleaned.to_csv(\"review.csv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12862ff3ca9680187dad6d9e1eb7c65a968b3aaddcd4032c5db42f20159eda4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
